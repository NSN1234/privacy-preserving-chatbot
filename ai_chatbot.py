# -*- coding: utf-8 -*-
"""AI-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yNEFQ4Y09pau-eFgNadDBO5aA1jn0nsg
"""

!wget -O mistral-7b-instruct-v0.1.Q4_K_M.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf

!pip install -U langchain-community

!pip install pdfplumber langchain faiss-cpu llama-cpp-python sentence-transformers

!pip install pytesseract pdf2image pillow tesseract



!apt-get install -y poppler-utils

!apt-get install -y tesseract-ocr
!pip install pytesseract

!pip install chromadb

import os
import pdfplumber
import re
import html
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import LlamaCpp

# ✅ **Use Mistral-7B instead of LLaMA**
MODEL_PATH = "/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

if not os.path.exists(MODEL_PATH):
    raise ValueError(f"Model path does not exist: {MODEL_PATH}. Please provide the correct model file.")

# ✅ **PDF Processing with OCR Support**
def extract_pdf_content(pdf_path):
    content = {"text": [], "tables": [], "images": [], "pages": []}

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                content["text"].append(f"Page {page_num+1}:\n{text}")

            tables = page.extract_tables()
            for table_num, table in enumerate(tables):
                md_table = "\n".join(["|".join([cell if cell else "" for cell in row]) for row in table])
                content["tables"].append(f"Page {page_num+1} Table {table_num+1}:\n{md_table}")

            content["pages"].append(page_num + 1)

    # OCR for scanned PDFs
    images = convert_from_path(pdf_path)
    for idx, image in enumerate(images):
        text = pytesseract.image_to_string(image)
        if text.strip():
            content["images"].append(f"OCR Page {idx+1}:\n{text}")

    return content

# ✅ **Better Chunk Processing**
def process_content(content):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # ✅ Larger chunk size for more context
        chunk_overlap=300,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = []
    for text in content["text"]:
        chunks.extend(text_splitter.split_text(text))

    for table in content["tables"]:
        chunks.append(f"TABLE DATA:\n{table}")

    for img_text in content["images"]:
        chunks.append(f"IMAGE TEXT:\n{img_text}")

    return chunks

# ✅ **Embedding and Vector Store Using ChromaDB**
def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_texts(chunks, embeddings)
    return vector_store

# ✅ **Query Processing with Exact References**
def answer_question(query, vector_store, llm):
    docs = vector_store.similarity_search(query, k=3)

    context = []
    sources = []
    for doc in docs:
        content = doc.page_content
        if "TABLE DATA:" in content:
            context.append(f"[Table Context]\n{content}")
        elif "IMAGE TEXT:" in content:
            context.append(f"[OCR Extracted Text]\n{content}")
        else:
            context.append(f"[Text Context]\n{content}")

        page_match = re.search(r"Page (\d+)", content)
        if page_match:
            sources.append(f"Page {page_match.group(1)}")

    prompt = f"""Answer based on context:
{''.join(context)}

Question: {query}
Include source pages: {', '.join(set(sources)) if sources else 'Not found'}
Answer:"""

    response = llm(prompt)

    # ✅ **Append Exact References Below the Answer**
    references = "\n\n---\n**Extracted References from PDF:**\n" + "\n".join(context)
    return response + references

# ✅ **Load Mistral-7B GGUF Model**
llm = LlamaCpp(
    model_path=MODEL_PATH,
    temperature=0.1,
    n_ctx=2048,
    n_batch=512
)

# ✅ **Run Workflow**
def process_query(pdf_paths, query):
    all_chunks = []
    for path in pdf_paths:
        content = extract_pdf_content(path)
        all_chunks.extend(process_content(content))

    vector_store = create_vector_store(all_chunks)
    return answer_question(query, vector_store, llm)

# ✅ **Example Usage**
result = process_query(
    ["/content/pdf-1.pdf"],
    "Explain what are symmetrically opposite models"
)
print(result)

import os
import pdfplumber
import re
import html
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import LlamaCpp

# ✅ **Use Mistral-7B instead of LLaMA**
MODEL_PATH = "/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

if not os.path.exists(MODEL_PATH):
    raise ValueError(f"Model path does not exist: {MODEL_PATH}. Please provide the correct model file.")

# ✅ **PDF Processing with OCR Support**
def extract_pdf_content(pdf_path):
    content = {"text": [], "tables": [], "images": [], "pages": []}

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                content["text"].append(f"Page {page_num+1}:\n{text}")

            tables = page.extract_tables()
            for table_num, table in enumerate(tables):
                md_table = "\n".join(["|".join([cell if cell else "" for cell in row]) for row in table])
                content["tables"].append(f"Page {page_num+1} Table {table_num+1}:\n{md_table}")

            content["pages"].append(page_num + 1)

    # OCR for scanned PDFs
    images = convert_from_path(pdf_path)
    for idx, image in enumerate(images):
        text = pytesseract.image_to_string(image)
        if text.strip():
            content["images"].append(f"OCR Page {idx+1}:\n{text}")

    return content

# ✅ **Better Chunk Processing**
def process_content(content):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # ✅ Larger chunk size for more context
        chunk_overlap=300,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = []
    for text in content["text"]:
        chunks.extend(text_splitter.split_text(text))

    for table in content["tables"]:
        chunks.append(f"TABLE DATA:\n{table}")

    for img_text in content["images"]:
        chunks.append(f"IMAGE TEXT:\n{img_text}")

    return chunks

# ✅ **Embedding and Vector Store Using ChromaDB**
def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_texts(chunks, embeddings)
    return vector_store

# ✅ **Query Processing with Exact References**
def answer_question(query, vector_store, llm):
    docs = vector_store.similarity_search(query, k=3)

    context = []
    sources = []
    for doc in docs:
        content = doc.page_content
        if "TABLE DATA:" in content:
            context.append(f"[Table Context]\n{content}")
        elif "IMAGE TEXT:" in content:
            context.append(f"[OCR Extracted Text]\n{content}")
        else:
            context.append(f"[Text Context]\n{content}")

        page_match = re.search(r"Page (\d+)", content)
        if page_match:
            sources.append(f"Page {page_match.group(1)}")

    prompt = f"""Answer based on context:
{''.join(context)}

Question: {query}
Include source pages: {', '.join(set(sources)) if sources else 'Not found'}
Answer:"""

    response = llm(prompt)

    # ✅ **Append Exact References Below the Answer**
    references = "\n\n---\n**Extracted References from PDF:**\n" + "\n".join(context)
    return response + references

# ✅ **Load Mistral-7B GGUF Model**
llm = LlamaCpp(
    model_path=MODEL_PATH,
    temperature=0.1,
    n_ctx=2048,
    n_batch=512
)

# ✅ **Run Workflow**
def process_query(pdf_paths, query):
    all_chunks = []
    for path in pdf_paths:
        content = extract_pdf_content(path)
        all_chunks.extend(process_content(content))

    vector_store = create_vector_store(all_chunks)
    return answer_question(query, vector_store, llm)

# ✅ **Example Usage**
result = process_query(
    ["/content/pdf-1.pdf"],
    "Give insights about revision control"
)
print(result)

import os
import pdfplumber
import re
import html
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import LlamaCpp

# ✅ **Use Mistral-7B instead of LLaMA**
MODEL_PATH = "/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

if not os.path.exists(MODEL_PATH):
    raise ValueError(f"Model path does not exist: {MODEL_PATH}. Please provide the correct model file.")

# ✅ **PDF Processing with OCR Support**
def extract_pdf_content(pdf_path):
    content = {"text": [], "tables": [], "images": [], "pages": []}

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                content["text"].append(f"Page {page_num+1}:\n{text}")

            tables = page.extract_tables()
            for table_num, table in enumerate(tables):
                md_table = "\n".join(["|".join([cell if cell else "" for cell in row]) for row in table])
                content["tables"].append(f"Page {page_num+1} Table {table_num+1}:\n{md_table}")

            content["pages"].append(page_num + 1)

    # OCR for scanned PDFs
    images = convert_from_path(pdf_path)
    for idx, image in enumerate(images):
        text = pytesseract.image_to_string(image)
        if text.strip():
            content["images"].append(f"OCR Page {idx+1}:\n{text}")

    return content

# ✅ **Better Chunk Processing**
def process_content(content):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # ✅ Larger chunk size for more context
        chunk_overlap=300,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = []
    for text in content["text"]:
        chunks.extend(text_splitter.split_text(text))

    for table in content["tables"]:
        chunks.append(f"TABLE DATA:\n{table}")

    for img_text in content["images"]:
        chunks.append(f"IMAGE TEXT:\n{img_text}")

    return chunks

# ✅ **Embedding and Vector Store Using ChromaDB**
def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_texts(chunks, embeddings)
    return vector_store

# ✅ **Query Processing with Exact References**
def answer_question(query, vector_store, llm):
    docs = vector_store.similarity_search(query, k=3)

    context = []
    sources = []
    for doc in docs:
        content = doc.page_content
        if "TABLE DATA:" in content:
            context.append(f"[Table Context]\n{content}")
        elif "IMAGE TEXT:" in content:
            context.append(f"[OCR Extracted Text]\n{content}")
        else:
            context.append(f"[Text Context]\n{content}")

        page_match = re.search(r"Page (\d+)", content)
        if page_match:
            sources.append(f"Page {page_match.group(1)}")

    prompt = f"""Answer based on context:
{''.join(context)}

Question: {query}
Include source pages: {', '.join(set(sources)) if sources else 'Not found'}
Answer:"""

    response = llm(prompt)

    # ✅ **Append Exact References Below the Answer**
    references = "\n\n---\n**Extracted References from PDF:**\n" + "\n".join(context)
    return response + references

# ✅ **Load Mistral-7B GGUF Model**
llm = LlamaCpp(
    model_path=MODEL_PATH,
    temperature=0.1,
    n_ctx=2048,
    n_batch=512
)

# ✅ **Run Workflow**
def process_query(pdf_paths, query):
    all_chunks = []
    for path in pdf_paths:
        content = extract_pdf_content(path)
        all_chunks.extend(process_content(content))

    vector_store = create_vector_store(all_chunks)
    return answer_question(query, vector_store, llm)

# ✅ **Example Usage**
result = process_query(
    ["/content/pdf-1.pdf"],
    "Could you give the web address for data delivery between tooling suppliers and JLR?"
)
print(result)

!pip install pdf2image

import os
import pdfplumber
import re
import html
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import LlamaCpp

# ✅ **Use Mistral-7B instead of LLaMA**
MODEL_PATH = "/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

if not os.path.exists(MODEL_PATH):
    raise ValueError(f"Model path does not exist: {MODEL_PATH}. Please provide the correct model file.")

# ✅ **PDF Processing with OCR Support**
def extract_pdf_content(pdf_path):
    content = {"text": [], "tables": [], "images": [], "pages": []}

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                content["text"].append(f"Page {page_num+1}:\n{text}")

            tables = page.extract_tables()
            for table_num, table in enumerate(tables):
                md_table = "\n".join(["|".join([cell if cell else "" for cell in row]) for row in table])
                content["tables"].append(f"Page {page_num+1} Table {table_num+1}:\n{md_table}")

            content["pages"].append(page_num + 1)

    # OCR for scanned PDFs
    images = convert_from_path(pdf_path)
    for idx, image in enumerate(images):
        text = pytesseract.image_to_string(image)
        if text.strip():
            content["images"].append(f"OCR Page {idx+1}:\n{text}")

    return content

# ✅ **Better Chunk Processing**
def process_content(content):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # ✅ Larger chunk size for more context
        chunk_overlap=300,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = []
    for text in content["text"]:
        chunks.extend(text_splitter.split_text(text))

    for table in content["tables"]:
        chunks.append(f"TABLE DATA:\n{table}")

    for img_text in content["images"]:
        chunks.append(f"IMAGE TEXT:\n{img_text}")

    return chunks

# ✅ **Embedding and Vector Store Using ChromaDB**
def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_texts(chunks, embeddings)
    return vector_store

# ✅ **Query Processing with Exact References**
def answer_question(query, vector_store, llm):
    docs = vector_store.similarity_search(query, k=3)

    context = []
    sources = []
    for doc in docs:
        content = doc.page_content
        if "TABLE DATA:" in content:
            context.append(f"[Table Context]\n{content}")
        elif "IMAGE TEXT:" in content:
            context.append(f"[OCR Extracted Text]\n{content}")
        else:
            context.append(f"[Text Context]\n{content}")

        page_match = re.search(r"Page (\d+)", content)
        if page_match:
            sources.append(f"Page {page_match.group(1)}")

    prompt = f"""Answer based on context:
{''.join(context)}

Question: {query}
Include source pages: {', '.join(set(sources)) if sources else 'Not found'}
Answer:"""

    response = llm(prompt)

    # ✅ **Append Exact References Below the Answer**
    references = "\n\n---\n**Extracted References from PDF:**\n" + "\n".join(context)
    return response + references

# ✅ **Load Mistral-7B GGUF Model**
llm = LlamaCpp(
    model_path=MODEL_PATH,
    temperature=0.1,
    n_ctx=2048,
    n_batch=512
)

# ✅ **Run Workflow**
def process_query(pdf_paths, query):
    all_chunks = []
    for path in pdf_paths:
        content = extract_pdf_content(path)
        all_chunks.extend(process_content(content))

    vector_store = create_vector_store(all_chunks)
    return answer_question(query, vector_store, llm)

# ✅ **Example Usage**
result = process_query(
    ["/content/pdf_1.pdf"],
    "Could you give the web address for data delivery between tooling suppliers and JLR?"
)
print(result)

import os
import pdfplumber
import re
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import LlamaCpp

# ✅ **Use Mistral-7B instead of LLaMA**
MODEL_PATH = "/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
if not os.path.exists(MODEL_PATH):
    raise ValueError(f"Model path does not exist: {MODEL_PATH}. Please provide the correct model file.")

# ✅ **PDF Processing with OCR & Full Images**
def extract_pdf_content(pdf_path):
    content = {"text": [], "tables": [], "images": []}
    images_list = []

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                content["text"].append(f"Page {page_num+1}:\n{text}")

            tables = page.extract_tables()
            for table_num, table in enumerate(tables):
                md_table = "\n".join(["|".join([cell if cell else "" for cell in row]) for row in table])
                content["tables"].append(f"Page {page_num+1} Table {table_num+1}:\n{md_table}")

    # Convert PDF to images
    images = convert_from_path(pdf_path)
    for idx, image in enumerate(images):
        text = pytesseract.image_to_string(image).strip()
        content["images"].append(f"OCR Page {idx+1}:\n{text}")
        images_list.append(image)  # Store full image

    return content, images_list

# ✅ **Semantic Chunking for Better Retrieval**
def process_content(content):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # ✅ Larger chunk size for more context
        chunk_overlap=300,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = []
    for text in content["text"]:
        chunks.extend(text_splitter.split_text(text))

    for table in content["tables"]:
        chunks.append(f"TABLE DATA:\n{table}")

    for img_text in content["images"]:
        chunks.append(f"IMAGE TEXT:\n{img_text}")

    return chunks

# ✅ **Embedding and Vector Store Using ChromaDB**
def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_texts(chunks, embeddings)
    return vector_store

# ✅ **Query Processing with Image Display**
def answer_question(query, vector_store, llm, images_list):
    docs = vector_store.similarity_search(query, k=3)

    context = []
    sources = []
    for doc in docs:
        content = doc.page_content
        if "TABLE DATA:" in content:
            context.append(f"[Table Context]\n{content}")
        elif "IMAGE TEXT:" in content:
            context.append(f"[OCR Extracted Text]\n{content}")
        else:
            context.append(f"[Text Context]\n{content}")

        page_match = re.search(r"Page (\d+)", content)
        if page_match:
            sources.append(f"Page {page_match.group(1)}")

    prompt = f"""Answer based on context:
{''.join(context)}

Question: {query}
Include source pages: {', '.join(set(sources)) if sources else 'Not found'}
Answer:"""

    response = llm(prompt)

    # ✅ **Append Extracted References Below the Answer**
    references = "\n\n---\n**Extracted References from PDF:**\n" + "\n".join(context)

    # ✅ **Display images after LLM response**
    print(response + references)
    for image in images_list:
        image.show()

    return response + references

# ✅ **Load Mistral-7B GGUF Model**
llm = LlamaCpp(
    model_path=MODEL_PATH,
    temperature=0.1,
    n_ctx=2048,
    n_batch=512
)

# ✅ **Run Workflow**
def process_query(pdf_paths, query):
    all_chunks = []
    all_images = []

    for path in pdf_paths:
        content, images_list = extract_pdf_content(path)
        all_chunks.extend(process_content(content))
        all_images.extend(images_list)

    vector_store = create_vector_store(all_chunks)
    return answer_question(query, vector_store, llm, all_images)

# ✅ **Example Usage**
result = process_query(
    ["/content/pdf_1.pdf"],
    "Could you give me the "
)

